{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to increase the size of the dataset and see if it improves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data set\n",
    "# --------------------\n",
    "\n",
    "def read_text(path):\n",
    "  with open(path, 'r') as f:\n",
    "    return f.read()\n",
    "  \n",
    "text_paths = [\n",
    "  \"../a06_RNN_language_model/animal_farm.txt\",\n",
    "  \"../a22_transformer_larger_text/books/alice-s-adventures-in-wonderland.txt\",\n",
    "]\n",
    "\n",
    "texts = [read_text(path) for path in text_paths]\n",
    "text_train = ''.join(texts)\n",
    "\n",
    "# Create list of unique characters\n",
    "vocab = sorted(list(set(text_train)))\n",
    "\n",
    "# Create dictionaries that map characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_char = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "# Convert text to integers\n",
    "train_data = torch.tensor([char_to_int[c] for c in text_train])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 50\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "  \n",
    "  \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 30\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "  \"\"\"\n",
    "  Args:\n",
    "      source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "      i: int\n",
    "\n",
    "  Returns:\n",
    "      tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "      target has shape ``[seq_len * batch_size]``\n",
    "  \"\"\"\n",
    "  seq_len = min(bptt, len(source) - 1 - i)\n",
    "  data = source[i:i+seq_len]\n",
    "  target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "  return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 4  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001  # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets) \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / (batch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "start_time = time.time()\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model)\n",
    "    print(f'Epoch {epoch:3d}: {train_loss:5.2f}')\n",
    "\n",
    "\n",
    "print(f\"Training time: {round(time.time() - start_time)} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model, initial_text, n_elements, temperature=1.0):\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  \n",
    "  initial_sequence = [char_to_int[c] for c in initial_text]\n",
    "  generated_sequence = initial_sequence\n",
    "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "  \n",
    "  # Use torch.no_grad() to prevent gradient calculations during text generation\n",
    "  with torch.no_grad():\n",
    "    # Generate new elements\n",
    "    for _ in range(n_elements):\n",
    "      # Convert the input_sequence to a tensor and add batch dimension\n",
    "      input_tensor = torch.tensor(generated_sequence[-bptt:]).unsqueeze(1).to(device)\n",
    "      seq_len = input_tensor.size(0)\n",
    "      \n",
    "      if seq_len != bptt:\n",
    "        src_mask = src_mask[:seq_len, :seq_len]\n",
    "    \n",
    "      # Evaluate the model\n",
    "      output = model(input_tensor, src_mask)\n",
    "      \n",
    "                        \n",
    "      # Apply temperature scaling to the output logits to control the randomness of the generated text\n",
    "      output = output[-1, 0, :] / temperature\n",
    "            \n",
    "      # Convert the output logits into probabilities using softmax\n",
    "      probabilities = torch.softmax(output, dim=-1)\n",
    "          \n",
    "      # Sample the next element using the probabilities\n",
    "      next_element = torch.multinomial(probabilities, num_samples=1).item()\n",
    "      \n",
    "      # Append the element\n",
    "      generated_sequence += [next_element]\n",
    "\n",
    "  generated_text = [int_to_char[c] for c in generated_sequence]\n",
    "  return ''.join(generated_text)\n",
    "\n",
    "generated = generate_sequence(model, \"We are not like that\", 300, 1)\n",
    "print(generated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample output\n",
    "\n",
    "We are not like that others, a bid.\n",
    "\n",
    "Snowball starce, when sheep where that was olders did never mining in a clisloar.\n",
    "\n",
    "Own from and the nured the arest with an before, they were her, and round, it the rubblough!\n",
    "\n",
    "From though not rade had been full one the lies.\n",
    "\n",
    "And a would produced and the all momes as and other othe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
