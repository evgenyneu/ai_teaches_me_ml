{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to remind you, the purpose of this is for me to learn machine learning, and building language models, in particular. Let's move on. What do you think I should learn next? Come up with a plan for a 60 minute session. Remember I like hands-on approach where I learn by coding in my Jupyter notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job on completing the RNN exercise! For your next 60-minute session, I suggest you dive into learning about the Transformer architecture, which is a more advanced and efficient approach to handling language-related tasks. Here's a plan for a hands-on session:\n",
    "\n",
    "* Introduction to Transformers (5 minutes):\n",
    "    * Briefly discuss the limitations of RNNs (like difficulty in handling long-range dependencies) and the advantages of Transformers (like parallelization and self-attention mechanism).\n",
    "\n",
    "* Understanding Self-Attention Mechanism (10 minutes):\n",
    "    * Explain the concept of self-attention and how it helps in capturing long-range dependencies.\n",
    "    * Provide a simple example to illustrate the mechanism.\n",
    "\n",
    "* Implementing Multi-Head Attention (15 minutes):\n",
    "    * Write code to implement the multi-head attention mechanism using PyTorch in your Jupyter notebook.\n",
    "\n",
    "* Creating the Transformer Architecture (20 minutes):\n",
    "    * Implement the Transformer architecture in your Jupyter notebook, including the encoder and decoder blocks, and the overall architecture.\n",
    "    * Set up the training process for a toy task, such as text classification or machine translation.\n",
    "\n",
    "* Training and Evaluating the Transformer Model (5 minutes):\n",
    "    * Train the Transformer model on the toy task and evaluate its performance.\n",
    "    * Compare the performance with the RNN model you built earlier.\n",
    "\n",
    "* Discussing Further Improvements and Next Steps (5 minutes):\n",
    "    * Briefly discuss the possible improvements to the Transformer model, such as fine-tuning, using pre-trained models (e.g., BERT, GPT-2), and exploring more complex tasks.\n",
    "    * Outline potential next steps for further learning, such as learning about other advanced NLP techniques or diving into specific Transformer-based models like BERT or GPT-3.\n",
    "\n",
    "Remember to add code cells and markdown explanations in your Jupyter notebook to guide you through each step. Enjoy your hands-on learning experience with Transformers!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a brief introduction to Transformers and their advantages over RNNs.\n",
    "\n",
    "## 1. Introduction to Transformers\n",
    "\n",
    "Transformers were introduced by Vaswani et al. in the paper \"Attention is All You Need\" in 2017. They are a type of neural network architecture specifically designed for handling sequences, like RNNs, but with some key differences and improvements.\n",
    "\n",
    "The main limitations of RNNs include difficulty in handling long-range dependencies and sequential processing of input data, which makes them slower to train. Transformers address these issues with the following key innovations:\n",
    "\n",
    "* Self-Attention Mechanism: Transformers use a self-attention mechanism that allows them to weigh the importance of different input elements in the sequence relative to each other. This mechanism helps capture long-range dependencies and relationships between input elements more effectively.\n",
    "\n",
    "* Parallelization: Unlike RNNs, which process input data sequentially, Transformers can process input data in parallel, allowing for faster training on modern hardware like GPUs.\n",
    "\n",
    "Now, let's dive into the self-attention mechanism.\n",
    "\n",
    "## 2. Understanding Self-Attention Mechanism\n",
    "\n",
    "Self-attention is a mechanism that allows the model to weigh the importance of each element in the input sequence relative to every other element. It computes a weighted sum of the input elements, where the weights are determined by a dot product between the query, key, and value vectors.\n",
    "\n",
    "Here's a simple example to illustrate the self-attention mechanism:\n",
    "\n",
    "Suppose you have the sentence: \"The cat jumped over the dog.\"\n",
    "Imagine the model needs to focus on the word \"cat\" to understand the subject of the sentence. The self-attention mechanism allows the model to assign higher weights to the words that are more relevant to \"cat\" (like \"jumped\" and \"over\") and lower weights to the less relevant words (like \"the\" and \"dog\").\n",
    "\n",
    "## 3. Implementing Multi-Head Attention \n",
    "let's move on to implementing the multi-head attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    assert d_model % num_heads == 0\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_model // num_heads\n",
    "\n",
    "    self.query_linear = nn.Linear(d_model, d_model)\n",
    "    self.key_linear = nn.Linear(d_model, d_model)\n",
    "    self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.attention = nn.Softmax(dim=-1)\n",
    "    self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def forward(self, query, key, value, mask=None):\n",
    "    batch_size = query.shape[0]\n",
    "\n",
    "    # Linear projections\n",
    "    query = self.query_linear(query)\n",
    "    key = self.key_linear(key)\n",
    "    value = self.value_linear(value)\n",
    "\n",
    "    # Split into multiple heads\n",
    "    query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attention_weights = self.attention(scores)\n",
    "    context = torch.matmul(attention_weights, value)\n",
    "\n",
    "    # Concatenate heads and pass through the output linear layer\n",
    "    context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "    output = self.output_linear(context)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented the multi-head attention mechanism, let's move on to building a simple Transformer model.\n",
    "\n",
    "## 4. Building a Simple Transformer Model:\n",
    "\n",
    "A Transformer model consists of an encoder and a decoder. The encoder processes the input sequence, and the decoder generates the output sequence. Both the encoder and the decoder are composed of multiple layers, each containing a multi-head self-attention mechanism and a position-wise feed-forward network.\n",
    "\n",
    "Here's a minimal implementation of a simple Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, hidden_dim, dropout):\n",
    "    super(TransformerEncoderLayer, self).__init__()\n",
    "    self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(d_model, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(dropout),\n",
    "      nn.Linear(hidden_dim, d_model)\n",
    "    )\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "    x = self.norm1(x + self.dropout1(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x + self.dropout2(ff_output))\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you can focus on understanding and implementing the encoder part of the Transformer model. You can refer to the original Transformer paper and various tutorials for implementing the complete Transformer model with encoder and decoder for specific tasks like language translation or text generation.\n",
    "\n",
    "Now, spend the remaining time experimenting with the code and trying different configurations. Make sure you understand each part of the code and how they connect to each other. You can also explore different applications of Transformers, such as text classification or named entity recognition, and try implementing them with the simple Transformer model you've built.\n",
    "\n",
    "Feel free to ask any questions or clarify any doubts you may have during this hands-on session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ok, it does not make much sense to me yet. I think it will get clearer once we build and train, and use the model. Let's continue with the lesson plan. Reminder we need to to the second part of section 4 Set up the training process for a toy task, such as text classification or machine translation. For a task, I suggest a text completion, same one we did with RNN earlier. I already have a text for animal_farm.txt we can use for training. Let's do the code from scratch. Please answer with code, but one step at a time (i.e. not multiple code blocks in one response.\n",
    "\n",
    "sure, let's start with setting up the data preprocessing for the text completion task using the Transformer model. We will first tokenize the input text, convert it into numerical indices and create input-target pairs.\n",
    "\n",
    "## Tokenize the text and create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext import vocab as torchtext_vocab\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return [c for c in text.lower()]\n",
    "\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "tokens = tokenize_text(text)\n",
    "counter = Counter(tokens)\n",
    "vocab = torchtext_vocab.Vocab(counter)\n",
    "\n",
    "PAD_IDX = vocab['<pad>']\n",
    "UNK_IDX = vocab['<unk>']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully created the vocab, let's move on to the next step.\n",
    "\n",
    "## Create the input and target sequences\n",
    "\n",
    "Since you want to use the same text completion task as in the RNN lesson, you can follow a similar process to create input and target sequences. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokens, seq_length):\n",
    "  input_sequences = []\n",
    "  target_sequences = []\n",
    "  \n",
    "  for i in range(len(tokens) - seq_length):\n",
    "    input_sequence = tokens[i : i + seq_length]\n",
    "    target_sequence = tokens[i + 1 : i + seq_length + 1]\n",
    "    input_sequences.append([vocab[t] for t in input_sequence])\n",
    "    target_sequences.append([vocab[t] for t in target_sequence])\n",
    "    \n",
    "  return input_sequences, target_sequences\n",
    "\n",
    "seq_length = 100\n",
    "input_sequences, target_sequences = create_sequences(tokens, seq_length)\n",
    "\n",
    "# Convert the input and target sequences into tensors\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(input_sequences, dtype=torch.long)\n",
    "y = torch.tensor(target_sequences, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
