{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to remind you, the purpose of this is for me to learn machine learning, and building language models, in particular. Let's move on. What do you think I should learn next? Come up with a plan for a 60 minute session. Remember I like hands-on approach where I learn by coding in my Jupyter notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job on completing the RNN exercise! For your next 60-minute session, I suggest you dive into learning about the Transformer architecture, which is a more advanced and efficient approach to handling language-related tasks. Here's a plan for a hands-on session:\n",
    "\n",
    "* Introduction to Transformers (5 minutes):\n",
    "    * Briefly discuss the limitations of RNNs (like difficulty in handling long-range dependencies) and the advantages of Transformers (like parallelization and self-attention mechanism).\n",
    "\n",
    "* Understanding Self-Attention Mechanism (10 minutes):\n",
    "    * Explain the concept of self-attention and how it helps in capturing long-range dependencies.\n",
    "    * Provide a simple example to illustrate the mechanism.\n",
    "\n",
    "* Implementing Multi-Head Attention (15 minutes):\n",
    "    * Write code to implement the multi-head attention mechanism using PyTorch in your Jupyter notebook.\n",
    "\n",
    "* Creating the Transformer Architecture (20 minutes):\n",
    "    * Implement the Transformer architecture in your Jupyter notebook, including the encoder and decoder blocks, and the overall architecture.\n",
    "    * Set up the training process for a toy task, such as text classification or machine translation.\n",
    "\n",
    "* Training and Evaluating the Transformer Model (5 minutes):\n",
    "    * Train the Transformer model on the toy task and evaluate its performance.\n",
    "    * Compare the performance with the RNN model you built earlier.\n",
    "\n",
    "* Discussing Further Improvements and Next Steps (5 minutes):\n",
    "    * Briefly discuss the possible improvements to the Transformer model, such as fine-tuning, using pre-trained models (e.g., BERT, GPT-2), and exploring more complex tasks.\n",
    "    * Outline potential next steps for further learning, such as learning about other advanced NLP techniques or diving into specific Transformer-based models like BERT or GPT-3.\n",
    "\n",
    "Remember to add code cells and markdown explanations in your Jupyter notebook to guide you through each step. Enjoy your hands-on learning experience with Transformers!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a brief introduction to Transformers and their advantages over RNNs.\n",
    "\n",
    "## 1. Introduction to Transformers\n",
    "\n",
    "Transformers were introduced by Vaswani et al. in the paper \"Attention is All You Need\" in 2017. They are a type of neural network architecture specifically designed for handling sequences, like RNNs, but with some key differences and improvements.\n",
    "\n",
    "The main limitations of RNNs include difficulty in handling long-range dependencies and sequential processing of input data, which makes them slower to train. Transformers address these issues with the following key innovations:\n",
    "\n",
    "* Self-Attention Mechanism: Transformers use a self-attention mechanism that allows them to weigh the importance of different input elements in the sequence relative to each other. This mechanism helps capture long-range dependencies and relationships between input elements more effectively.\n",
    "\n",
    "* Parallelization: Unlike RNNs, which process input data sequentially, Transformers can process input data in parallel, allowing for faster training on modern hardware like GPUs.\n",
    "\n",
    "Now, let's dive into the self-attention mechanism.\n",
    "\n",
    "## 2. Understanding Self-Attention Mechanism\n",
    "\n",
    "Self-attention is a mechanism that allows the model to weigh the importance of each element in the input sequence relative to every other element. It computes a weighted sum of the input elements, where the weights are determined by a dot product between the query, key, and value vectors.\n",
    "\n",
    "Here's a simple example to illustrate the self-attention mechanism:\n",
    "\n",
    "Suppose you have the sentence: \"The cat jumped over the dog.\"\n",
    "Imagine the model needs to focus on the word \"cat\" to understand the subject of the sentence. The self-attention mechanism allows the model to assign higher weights to the words that are more relevant to \"cat\" (like \"jumped\" and \"over\") and lower weights to the less relevant words (like \"the\" and \"dog\")."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = text[:2000] # make the text shorter for testing\n",
    "\n",
    "# Create liar of unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Create dictionaries that map characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Convert text to integers\n",
    "int_text = [char_to_int[c] for c in text]\n",
    "\n",
    "# Create input and target sequences\n",
    "sequence_length = 50\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "# Convert lists to tensors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformer model\n",
    "\n",
    "Now that we have our input and target sequences, let's create the Transformer model for text completion. We'll use the TransformerEncoderLayer in our model, and add an embedding layer and a linear output layer to generate character probabilities.\n",
    "\n",
    "This TransformerModel class defines our complete model for text completion. It has an embedding layer to convert input sequences into continuous vectors, a transformer encoder that takes the embedded inputs, and an output layer that produces probabilities for each character in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, num_heads, hidden_dim, num_layers, dropout):\n",
    "    super(TransformerModel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    self.transformer_encoder = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model, num_heads, hidden_dim, dropout),\n",
    "      num_layers\n",
    "    )\n",
    "    self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Transformer model\n",
    "\n",
    "Now, let's instantiate the Transformer model, set the hyperparameters, and define the loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(chars)\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "hidden_dim = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 32  # Reduce the batch size further\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, d_model, num_heads, num_layers, hidden_dim, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use the CrossEntropyLoss as our loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now that we have the model, loss function, and optimizer defined, let's create the training loop for our model. We will train the model for a specified number of epochs, update the model's weights, and print the training loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = len(X) // batch_size \n",
    "print('total_iterations', total_iterations)\n",
    "min_per_epoch = total_iterations * 2 / 330\n",
    "print('min_per_epoch', min_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, input_data, target_data):\n",
    "    self.input_data = input_data\n",
    "    self.target_data = target_data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_data[idx], self.target_data[idx]\n",
    "      \n",
    "\n",
    "# Create the dataset\n",
    "text_dataset = TextDataset(X, y)\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_batch)\n",
    "    loss = loss_function(output.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  # Print the average loss for this epoch\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(data_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
