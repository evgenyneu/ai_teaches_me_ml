{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data. They can maintain a hidden state that gets updated at each step in the sequence, making them well-suited for tasks like language modeling.\n",
    "\n",
    "For our simple language model, we'll use a character-level RNN. It will predict the next character in a sequence given the previous characters. This is a simpler task compared to predicting the next word, as GPT models do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's begin with preparing the dataset.\n",
    "\n",
    "## Prepare the dataset\n",
    "\n",
    "For our simple language model, we'll use a text file as our dataset. You can choose any text file you like, such as a book from Project Gutenberg or a simple text file with some sample sentences.\n",
    "\n",
    "Follow these steps to prepare the dataset:\n",
    "\n",
    "* Load the text file and preprocess it (lowercase, remove special characters, etc.)\n",
    "* Create a dictionary that maps characters to integers and another dictionary that maps integers to characters. We'll use these dictionaries to convert the text to numbers and vice versa.\n",
    "* Convert the text to a sequence of integers using the character-to-integer dictionary.\n",
    "* Create input and target sequences. For each sequence of length n in the input, the corresponding target sequence should have the same length and contain the next character in the text for each character in the input sequence.\n",
    "\n",
    "Here's some code to help you get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the text\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[0:5000] # Make text shorter for faster testing\n",
    "\n",
    "# Create dictionaries\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Convert text to integers\n",
    "int_text = [char_to_int[c] for c in text]\n",
    "\n",
    "# Create input and target sequences\n",
    "sequence_length = 50\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the RNN model in PyTorch\n",
    "\n",
    "We'll create a simple RNN model using PyTorch's nn.RNN module along with a fully connected layer for the final output. Our model will have the following layers:\n",
    "\n",
    "* An embedding layer (nn.Embedding) to convert the input character integers to embeddings.\n",
    "* An RNN layer (nn.RNN) that will maintain hidden states and learn the sequence patterns.\n",
    "* A fully connected output layer (nn.Linear) that will produce the probabilities for the next character.\n",
    "\n",
    "Here's a simple implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "  def __init__(self, input_size, embed_size, hidden_size, output_size):\n",
    "    super(SimpleRNN, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, embed_size)\n",
    "    self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    x, hidden = self.rnn(x, hidden)\n",
    "    x = self.fc(x)\n",
    "    return x, hidden\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "input_size = len(chars)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "\n",
    "model = SimpleRNN(input_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and the loss function\n",
    "\n",
    "Now that we have our RNN model defined, we need to set up the training process. We'll need a loss function, an optimizer, and a training loop that feeds the input sequences and target sequences to the model, computes the loss, and updates the model's parameters.\n",
    "\n",
    "First, let's define the loss function and the optimizer. We'll use the Cross Entropy Loss, which is suitable for classification tasks, and the Adam optimizer for updating the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Next, let's create the training loop. We'll need to perform the following steps for each epoch:\n",
    "\n",
    "* Reset the hidden state using model.init_hidden().\n",
    "* Pass the input sequence and the initial hidden state to the model.\n",
    "* Compute the loss between the model's output and the target sequence.\n",
    "* Backpropagate the loss and update the model's parameters using the optimizer.\n",
    "\n",
    "Here's the code for the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Set the device\n",
    "# this allows to run the code on GPU if available to make it faster\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 64\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "  start_epoch = time.time()\n",
    "  \n",
    "  # Loop over the input-target pairs in the dataset\n",
    "  for i in range(0, len(X), batch_size):\n",
    "    # Get the actual batch size for the current iteration\n",
    "    actual_batch_size = min(batch_size, len(X) - i)\n",
    "    \n",
    "    # Reset the hidden state\n",
    "    hidden = model.init_hidden(actual_batch_size).to(device)\n",
    "    \n",
    "    # Detach the hidden state from its history\n",
    "    hidden.detach_()\n",
    "    \n",
    "    # Get a batch of input and target sequences\n",
    "    input_batch = X[i:i+actual_batch_size]\n",
    "    target_batch = y[i:i+actual_batch_size]\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: pass the input and hidden state to the model\n",
    "    output, hidden = model(input_batch, hidden)\n",
    "    \n",
    "    # Reshape the output and target_batch\n",
    "    output = output.view(-1, output.shape[2])\n",
    "    target_batch = target_batch.view(-1)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    # Backward pass: compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    start = time.time()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Print the loss for this epoch\n",
    "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, time: {time.time() - start_epoch:.2f} s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "As you gain more experience and work with larger datasets, you can consider using more powerful hardware, like GPUs, and optimize the code further to improve training time.\n",
    "\n",
    "Now that you've trained the model, you can use it to generate new text by providing it with an initial sequence and sampling the output probabilities. This will allow you to see how well the model has learned the structure and style of the input text.\n",
    "\n",
    "You can use the following function to generate new text using your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, initial_sequence, n_chars, temperature=1.0):\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  # initial_sequence = initial_sequence.lower()  # Convert the initial sequence to lowercase\n",
    "  \n",
    "  # Only keep characters that are present in char_to_int\n",
    "  initial_sequence = ''.join(c for c in initial_sequence if c in char_to_int)\n",
    "  generated_sequence = initial_sequence\n",
    "\n",
    "  # Convert the initial sequence to a tensor\n",
    "  input_sequence = torch.tensor([char_to_int[c] for c in initial_sequence], dtype=torch.long).unsqueeze(1).to(device)\n",
    "  \n",
    "  # Initialize the hidden state\n",
    "  hidden = model.init_hidden(1).to(device)\n",
    "  \n",
    "  # Generate new characters\n",
    "  for _ in range(n_chars):\n",
    "    # Reshape input_sequence to have a sequence length of 1\n",
    "    input_sequence_reshaped = input_sequence.view(1, -1).to(device)\n",
    "    output, hidden = model(input_sequence_reshaped, hidden)\n",
    "\n",
    "    # Apply the temperature and sample the output probabilities\n",
    "    output_dist = output.data[0, -1, :].div(temperature).exp()\n",
    "    \n",
    "    top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "    # Add the generated character to the sequence\n",
    "    generated_char = int_to_char[top_i.item()]\n",
    "    generated_sequence += generated_char\n",
    "\n",
    "    # Append the number for the next predicted character to the input sequence\n",
    "    new_character = torch.tensor([[char_to_int[generated_char]]]).to(device)\n",
    "    input_sequence = torch.cat((input_sequence, new_character))\n",
    "\n",
    "  return generated_sequence\n",
    "  \n",
    "initial_sequence = \"We are not like that.\"\n",
    "n_chars = 300\n",
    "temperature = 1.0\n",
    "\n",
    "generated_text = generate_text(model, initial_sequence, n_chars, temperature)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "\n",
    "> We are not like that.\n",
    "\n",
    "Before long the of ours?\n",
    "\n",
    "Let us face it: our lives are miserable, laborious, and short.\n",
    "\n",
    "He wayd wast came the came mane, hoping to light in the big barn as soon as Mr. Jones, of the catto tame ant made a mare who drew Mr. Jonesâ€™stterep off, bro draw amo had not be trodden oft laskedMad ank in or"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
