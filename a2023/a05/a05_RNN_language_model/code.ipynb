{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It's nice to see that you already have some experience with neural networks and PyTorch. For today's lesson, we'll work on building a simple language model using an RNN in PyTorch.\n",
    "\n",
    "Here's the plan for our 60-minute session:\n",
    "\n",
    "* Briefly discuss RNNs (5 min)\n",
    "* Prepare the dataset (10 min)\n",
    "* Implement a simple RNN model in PyTorch (15 min)\n",
    "* Train the model (10 min)\n",
    "* Test the model (10 min)\n",
    "* Wrap up and discuss improvements (10 min)\n",
    "\n",
    "Let's get started with a brief discussion of RNNs.\n",
    "\n",
    "## 1. Briefly discuss RNNs (5 min)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data. They can maintain a hidden state that gets updated at each step in the sequence, making them well-suited for tasks like language modeling.\n",
    "\n",
    "For our simple language model, we'll use a character-level RNN. It will predict the next character in a sequence given the previous characters. This is a simpler task compared to predicting the next word, as GPT models do.\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "Please let me know if you are familiar with RNNs and if you have any questions about them. If you're ready to move on, I'll guide you through preparing the dataset for our language model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's begin with preparing the dataset.\n",
    "\n",
    "## 2. Prepare the dataset (10 min)\n",
    "\n",
    "For our simple language model, we'll use a text file as our dataset. You can choose any text file you like, such as a book from Project Gutenberg or a simple text file with some sample sentences.\n",
    "\n",
    "Follow these steps to prepare the dataset:\n",
    "\n",
    "* Load the text file and preprocess it (lowercase, remove special characters, etc.)\n",
    "* Create a dictionary that maps characters to integers and another dictionary that maps integers to characters. We'll use these dictionaries to convert the text to numbers and vice versa.\n",
    "* Convert the text to a sequence of integers using the character-to-integer dictionary.\n",
    "* Create input and target sequences. For each sequence of length n in the input, the corresponding target sequence should have the same length and contain the next character in the text for each character in the input sequence.\n",
    "\n",
    "Here's some code to help you get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the text\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "    text = ''.join(c for c in text if c.isalnum() or c.isspace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to integers\n",
    "int_text = [char_to_int[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and target sequences\n",
    "sequence_length = 50\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "    X.append(int_text[i:i + sequence_length])\n",
    "    y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's move on to implementing the RNN model.\n",
    "\n",
    "### 3. Implement a simple RNN model in PyTorch (15 min)\n",
    "\n",
    "We'll create a simple RNN model using PyTorch's nn.RNN module along with a fully connected layer for the final output. Our model will have the following layers:\n",
    "\n",
    "* An embedding layer (nn.Embedding) to convert the input character integers to embeddings.\n",
    "* An RNN layer (nn.RNN) that will maintain hidden states and learn the sequence patterns.\n",
    "* A fully connected output layer (nn.Linear) that will produce the probabilities for the next character.\n",
    "\n",
    "Here's a simple implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "  def __init__(self, input_size, embed_size, hidden_size, output_size):\n",
    "    super(SimpleRNN, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, embed_size)\n",
    "    self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    x, hidden = self.rnn(x, hidden)\n",
    "    x = self.fc(x)\n",
    "    return x, hidden\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "input_size = len(chars)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "\n",
    "model = SimpleRNN(input_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our RNN model defined, we need to set up the training process. We'll need a loss function, an optimizer, and a training loop that feeds the input sequences and target sequences to the model, computes the loss, and updates the model's parameters.\n",
    "\n",
    "First, let's define the loss function and the optimizer. We'll use the Cross Entropy Loss, which is suitable for classification tasks, and the Adam optimizer for updating the model's parameters.\n",
    "\n",
    "Add the following code to your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the training loop. We'll need to perform the following steps for each epoch:\n",
    "\n",
    "* Reset the hidden state using model.init_hidden().\n",
    "* Pass the input sequence and the initial hidden state to the model.\n",
    "* Compute the loss between the model's output and the target sequence.\n",
    "* Backpropagate the loss and update the model's parameters using the optimizer.\n",
    "\n",
    "Here's the code for the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target_batch)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Backward pass: compute the gradients\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m \u001b[39m# Update the model parameters\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/code/python/ai_teaches_me_ml/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/code/python/ai_teaches_me_ml/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Set the number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "  # Reset the hidden state\n",
    "  hidden = model.init_hidden(batch_size)\n",
    "  \n",
    "  # Loop over the input-target pairs in the dataset\n",
    "  # for i in range(0, len(X), batch_size):\n",
    "  for i in range(0, 300, batch_size):\n",
    "    # Get a batch of input and target sequences\n",
    "    input_batch = torch.tensor(X[i:i+batch_size], dtype=torch.long)\n",
    "    target_batch = torch.tensor(y[i:i+batch_size], dtype=torch.long)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: pass the input and hidden state to the model\n",
    "    output, hidden = model(input_batch, hidden)\n",
    "    \n",
    "    # Reshape the output and target_batch\n",
    "    output = output.view(-1, output.shape[2])\n",
    "    target_batch = target_batch.view(-1)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    # Backward pass: compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "  # # Print the loss for this epoch\n",
    "  # print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
