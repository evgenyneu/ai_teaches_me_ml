{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = text[:5000]  # make the text shorter for testing\n",
    "\n",
    "# Use the tokenizer to convert the text to tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "int_text = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create input and target sequences\n",
    "sequence_length = 300\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert lists to tensors\n",
    "X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, num_heads, hidden_dim, num_layers, dropout):\n",
    "    super(TransformerModel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoder = PositionalEncoding(d_model, dropout) \n",
    "    \n",
    "    self.transformer_encoder = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model, num_heads, hidden_dim, dropout),\n",
    "      num_layers\n",
    "    )\n",
    "    \n",
    "    self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    self.init_weights()\n",
    "    \n",
    "  def init_weights(self):\n",
    "    initrange = 0.1\n",
    "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    self.output_layer.bias.data.zero_()\n",
    "    self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoder(x)\n",
    "    x = self.transformer_encoder(x, mask=mask)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "hidden_dim = 128\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 16 \n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, d_model, num_heads, num_layers, hidden_dim, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use the CrossEntropyLoss as our loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, input_data, target_data):\n",
    "    self.input_data = input_data\n",
    "    self.target_data = target_data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_data[idx], self.target_data[idx]\n",
    "      \n",
    "      \n",
    "def generate_square_subsequent_mask(sz):\n",
    "  \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Create the dataset\n",
    "text_dataset = TextDataset(X, y)\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  start_epoch = time.time()\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    mask = generate_square_subsequent_mask(input_batch.size(0)).to(device)\n",
    "    output = model(input_batch, mask)\n",
    "    loss = loss_function(output.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  # Print the average loss for this epoch\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(data_loader):.4f}, time: {time.time() - start_epoch:.2f} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/5, Loss: 6.6443, time: 9.45 s\n",
    "Epoch 2/5, Loss: 5.4509, time: 8.87 s\n",
    "Epoch 3/5, Loss: 5.4365, time: 8.84 s\n",
    "Epoch 4/5, Loss: 5.4426, time: 8.92 s\n",
    "Epoch 5/5, Loss: 5.4368, time: 8.82 s\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_chars, temperature=1.0):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  # Convert the text into a sequence of integers using the char_to_int dictionary\n",
    "  tokens = tokenizer.tokenize(seed_text)\n",
    "  input_sequence = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  \n",
    "  # Use torch.no_grad() to prevent gradient calculations during text generation\n",
    "  with torch.no_grad():\n",
    "    # Generate 'num_chars' characters\n",
    "    for _ in range(num_chars):\n",
    "      input_tensor = torch.tensor([input_sequence[-sequence_length:]], dtype=torch.long).to(device)\n",
    "    \n",
    "      # Get the output probabilities for the next character by feeding the input tensor to the trained model\n",
    "      output = model(input_tensor)\n",
    "            \n",
    "      # Apply temperature scaling to the output logits to control the randomness of the generated text\n",
    "      output = output[:, -1, :] / temperature\n",
    "            \n",
    "      # Convert the output logits into probabilities using softmax\n",
    "      probabilities = torch.softmax(output, dim=-1)\n",
    "            \n",
    "      # Sample the index of the next character using the probabilities\n",
    "      next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "      input_sequence.append(next_token_id)\n",
    "      \n",
    "  # Generate text from the generated sequence of integers\n",
    "  generated_text = tokenizer.decode(input_sequence, clean_up_tokenization_spaces=True)\n",
    "  return generated_text\n",
    "\n",
    "seed_text = \"We are not like that.\"\n",
    "num_chars_to_generate = 300\n",
    "\n",
    "generated_text = generate_text(model, seed_text, num_chars_to_generate, temperature=0.8)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We are not like that. the.,, bed, a they and had time on da, never, and,ers was the the. the fact they. work.. he and as of theleg other., powers r the the, the not the and the barn of Old the was very, the, stripe at be keep the made he. the some, was character with the, s a,,er was think you the, some waschard the cows the, s remark raised all began, Major the hands. the the his front andush came fluttered together The of so in he the flies a the as thelover place place,, filed themselves Nevertheless, different was., - with, began, had that to I say had on the ordinary but Jessie flies the to pigeontive who, lump that sooner slai was the he all of animal Box,lover he duck, the to hairyevo in the openly universally on,, - of fact he The,ut After to. the he in last though not were was worst Major Jones a to, the asleeplings the a with was in the back, came the Major small the the and was C the s, just the and. with for and his wise that the of life and and a mare ready pu was had middle cart. he inside back the and - respected of,ew it was to somelover, barnd would put was Major onlent end the among back duck a the notme him,, chewing,s pigeon the any it a\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
