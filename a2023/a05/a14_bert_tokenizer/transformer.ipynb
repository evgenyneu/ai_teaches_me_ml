{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = text[:5000]  # make the text shorter for testing\n",
    "\n",
    "# Use the tokenizer to convert the text to tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "int_text = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create input and target sequences\n",
    "sequence_length = 300\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert lists to tensors\n",
    "X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):    \n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, num_heads, hidden_dim, num_layers, dropout):\n",
    "    super(TransformerModel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoder = PositionalEncoding(d_model, dropout) \n",
    "    \n",
    "    self.transformer_encoder = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model, num_heads, hidden_dim, dropout),\n",
    "      num_layers\n",
    "    )\n",
    "    \n",
    "    self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    self.init_weights()\n",
    "    \n",
    "  def init_weights(self):\n",
    "    initrange = 0.1\n",
    "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    self.output_layer.bias.data.zero_()\n",
    "    self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        src: Tensor, shape ``[seq_len, batch_size]``\n",
    "        src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "    Returns:\n",
    "        output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "    \"\"\"\n",
    "        \n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoder(x)\n",
    "    x = self.transformer_encoder(x, mask)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(vocab_size)\n",
    "print(len(set(int_text)))\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "hidden_dim = 128\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, d_model, num_heads, num_layers, hidden_dim, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use the CrossEntropyLoss as our loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, input_data, target_data):\n",
    "    self.input_data = input_data\n",
    "    self.target_data = target_data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_data[idx], self.target_data[idx]\n",
    "      \n",
    "      \n",
    "def generate_square_subsequent_mask(sz):\n",
    "  \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Create the dataset\n",
    "text_dataset = TextDataset(X, y)\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  start_epoch = time.time()\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    input_batch = input_batch.transpose(0, 1)  # Transpose the input\n",
    "    target_batch = target_batch.transpose(0, 1)  # Transpose the target\n",
    "\n",
    "    # Forward pass\n",
    "    mask = generate_square_subsequent_mask(sequence_length).to(device)\n",
    "    output = model(input_batch, mask)\n",
    "    loss = loss_function(output.reshape(-1, vocab_size), target_batch.reshape(-1))  # Flatten the outputs and targets\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  # Print the average loss for this epoch\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(data_loader):.4f}, time: {time.time() - start_epoch:.2f} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training output\n",
    "\n",
    "```\n",
    "Epoch 1/5, Loss: 7.4910, time: 9.43 s\n",
    "Epoch 2/5, Loss: 5.7837, time: 8.86 s\n",
    "Epoch 3/5, Loss: 5.7779, time: 8.87 s\n",
    "Epoch 4/5, Loss: 5.6356, time: 8.88 s\n",
    "Epoch 5/5, Loss: 5.6284, time: 8.90 s\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_chars, temperature=1.0):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  # Convert the text into a sequence of integers using the char_to_int dictionary\n",
    "  tokens = tokenizer.tokenize(seed_text)\n",
    "  input_sequence = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  \n",
    "  # Use torch.no_grad() to prevent gradient calculations during text generation\n",
    "  with torch.no_grad():\n",
    "    # Generate 'num_chars' characters\n",
    "    for _ in range(num_chars):\n",
    "      input_tensor = torch.tensor([input_sequence[-sequence_length:]], dtype=torch.long).to(device)\n",
    "    \n",
    "      # Get the output probabilities for the next character by feeding the input tensor to the trained model\n",
    "      output = model(input_tensor)\n",
    "            \n",
    "      # Apply temperature scaling to the output logits to control the randomness of the generated text\n",
    "      output = output[:, -1, :] / temperature\n",
    "            \n",
    "      # Convert the output logits into probabilities using softmax\n",
    "      probabilities = torch.softmax(output, dim=-1)\n",
    "            \n",
    "      # Sample the index of the next character using the probabilities\n",
    "      next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "      input_sequence.append(next_token_id)\n",
    "      \n",
    "  # Generate text from the generated sequence of integers\n",
    "  generated_text = tokenizer.decode(input_sequence, clean_up_tokenization_spaces=True)\n",
    "  return generated_text\n",
    "\n",
    "seed_text = \"We are not like that.\"\n",
    "num_chars_to_generate = 300\n",
    "\n",
    "generated_text = generate_text(model, seed_text, num_chars_to_generate, temperature=0.8)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample output\n",
    "\n",
    "```\n",
    "We are not like that.er and was and with squeezed the the man asleep Nevertheless front duck them, to, for cat,, it beast was, Benjamin had Mr which filed, at of itlings the, white usually, they so,.,llielings have tempered. to. a to, with to to Mo into the that wholings,ing to. to who a a duck white ’lover would. placench nearly s, Last and you White. s thelings not asleep foolish.,. who, Iedly Sundays would animals in doily God have came of big which was a saw a that Sundays., usually a would to da came which the.,. speaking white., their to the to the ; was who foolish the white of of. you nestled of, he Mr given the by them he laugh he the place to dide to it,ly theood be on it ;. trap of and animalsdock p did usually think that not Jones on raised to p tailbly, who a the to still lost his by a of., say who to inside to all the, of, of the,ing asleep, have at, and place t had a to of the sort, at animals of, Boxlings, devoted began She her at they and had, trap, got to down Nevertheless of to, not goat been never She dream to was at her for, to If immediately,,, in foring da and Sundays of my srod Jones not stripe asleep,ing laugh the\n",
    "```\n",
    "\n",
    "```\n",
    "We are not like that. have I the, he had the the side wall themselves the ae, Icing had on, fee last the place ribbons ’ took p man promptly it da. ’ a where lump saying and throughout to lumpcing the cat. man, with. they came and ’ drew the herself She, as ’ was foolish be to r door, moment squeezed usualbly in with ribbons strange Box C had for for he, say attention side. At. dream and to it and, had who last Word fell of never tobly the. youlover the,, of. place of made was farm or to that At find to and and stripe and. the But it as animals saw. down have, word, was in their Major and, had place, she the where. and not the. whitee,,lover, have of, the ; have began, to there be did asleep and a foolish fashion for throat to barn a, squeezed. comfortable. draw listening Major were thatrrede forilyint foolish, to finally and nestled years man down Mo inside that. looked throughout Mo the with, and the it pigeon who place saying they da, them understandradeaven.,, finally to present fore all and not pretty who. looked to that a the,, fee God to throughout had fee round asked, ’ted never that or herself, finally, Mr would where where don p the Jones, madetive nestled theing as a pu to I foolish they last who attention a Mom and\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
