{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = text[:5000]  # make the text shorter for testing\n",
    "\n",
    "# Use the tokenizer to convert the text to tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "int_text = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create input and target sequences\n",
    "sequence_length = 300\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert lists to tensors\n",
    "X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, num_heads, hidden_dim, num_layers, dropout):\n",
    "    super(TransformerModel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoder = PositionalEncoding(d_model, dropout) \n",
    "    \n",
    "    self.transformer_encoder = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model, num_heads, hidden_dim, dropout),\n",
    "      num_layers\n",
    "    )\n",
    "    \n",
    "    self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    self.init_weights()\n",
    "    \n",
    "  def init_weights(self):\n",
    "    initrange = 0.1\n",
    "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    self.output_layer.bias.data.zero_()\n",
    "    self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoder(x)\n",
    "    x = self.transformer_encoder(x, mask=mask)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "hidden_dim = 128\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 16 \n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, d_model, num_heads, num_layers, hidden_dim, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use the CrossEntropyLoss as our loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, input_data, target_data):\n",
    "    self.input_data = input_data\n",
    "    self.target_data = target_data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_data[idx], self.target_data[idx]\n",
    "      \n",
    "      \n",
    "def generate_square_subsequent_mask(sz):\n",
    "  \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Create the dataset\n",
    "text_dataset = TextDataset(X, y)\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  start_epoch = time.time()\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    mask = generate_square_subsequent_mask(input_batch.size(0)).to(device)\n",
    "    output = model(input_batch, mask)\n",
    "    loss = loss_function(output.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  # Print the average loss for this epoch\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(data_loader):.4f}, time: {time.time() - start_epoch:.2f} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/5, Loss: 6.6443, time: 9.45 s\n",
    "Epoch 2/5, Loss: 5.4509, time: 8.87 s\n",
    "Epoch 3/5, Loss: 5.4365, time: 8.84 s\n",
    "Epoch 4/5, Loss: 5.4426, time: 8.92 s\n",
    "Epoch 5/5, Loss: 5.4368, time: 8.82 s\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_chars, temperature=1.0):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  # Initialize the generated text with the seed text\n",
    "  generated_text = seed_text\n",
    "  \n",
    "  # Use torch.no_grad() to prevent gradient calculations during text generation\n",
    "  with torch.no_grad():\n",
    "    # Generate 'num_chars' characters\n",
    "    for _ in range(num_chars):\n",
    "      # Convert the current generated text into a sequence of integers using the char_to_int dictionary\n",
    "      tokens = tokenizer.tokenize(generated_text)\n",
    "      input_sequence = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      \n",
    "      # Convert the input_sequence to a tensor and add batch dimension\n",
    "      input_tensor = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "      \n",
    "      # Get the output probabilities for the next character by feeding the input tensor to the trained model\n",
    "      output = model(input_tensor)\n",
    "            \n",
    "      # Apply temperature scaling to the output logits to control the randomness of the generated text\n",
    "      output = output[:, -1, :] / temperature\n",
    "      \n",
    "      # Convert the output logits into probabilities using softmax\n",
    "      probabilities = torch.softmax(output, dim=-1)\n",
    "            \n",
    "      # Sample the index of the next character using the probabilities\n",
    "      next_char_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            \n",
    "      # Convert the index of the next character back to the character using the int_to_char dictionary\n",
    "      input_sequence += [next_char_idx]\n",
    "      \n",
    "      # Append the next character to the generated text\n",
    "      generated_text = tokenizer.decode(input_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "seed_text = \"We are not like that.\"\n",
    "num_chars_to_generate = 200\n",
    "\n",
    "generated_text = generate_text(model, seed_text, num_chars_to_generate, temperature=0.8)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We are not like that.,. for it side word heard had have themselves horses the for, filed on he the usually cut thein,,, in the. he say her, last he to a The came Mr and flies the,. openly intelligence of the mare was Benjamin as the the to powers the, old usually, he Box for he the,,, and, they not,. Mr who make themselves theird, an of, the, int had les, he,. -, lings, ing to raised and was should wise the. that c animals window, horses down back the great as in and flies. powers, immediately thatlover which though on, in her some many he in, two to came a never a Box underlover where ’,., theer place of Thes animal to, s way was, had, ’ would given themselves and he very, er, the he he the white At as was,. a draw be had tempered duck appearance him animals., red say\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
