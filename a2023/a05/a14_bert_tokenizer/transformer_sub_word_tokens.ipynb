{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "with open(\"../a06_RNN_language_model/animal_farm.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = text[:5000]  # make the text shorter for testing\n",
    "\n",
    "# Use the tokenizer to convert the text to tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "int_text = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create input and target sequences\n",
    "sequence_length = 300\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert lists to tensors\n",
    "X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):    \n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, num_heads, hidden_dim, num_layers, dropout):\n",
    "    super(TransformerModel, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoder = PositionalEncoding(d_model, dropout) \n",
    "    \n",
    "    self.transformer_encoder = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model, num_heads, hidden_dim, dropout),\n",
    "      num_layers\n",
    "    )\n",
    "    \n",
    "    self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    self.init_weights()\n",
    "    \n",
    "  def init_weights(self):\n",
    "    initrange = 0.1\n",
    "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    self.output_layer.bias.data.zero_()\n",
    "    self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        src: Tensor, shape ``[seq_len, batch_size]``\n",
    "        src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "    Returns:\n",
    "        output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "    \"\"\"\n",
    "        \n",
    "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "    x = self.pos_encoder(x)\n",
    "    x = self.transformer_encoder(x, mask)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(vocab_size)\n",
    "print(len(set(int_text)))\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "hidden_dim = 128\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, d_model, num_heads, num_layers, hidden_dim, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use the CrossEntropyLoss as our loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, input_data, target_data):\n",
    "    self.input_data = input_data\n",
    "    self.target_data = target_data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_data[idx], self.target_data[idx]\n",
    "      \n",
    "      \n",
    "def generate_square_subsequent_mask(sz):\n",
    "  \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Create the dataset\n",
    "text_dataset = TextDataset(X, y)\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  start_epoch = time.time()\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    input_batch = input_batch.transpose(0, 1)  # Transpose the input\n",
    "    target_batch = target_batch.transpose(0, 1)  # Transpose the target\n",
    "\n",
    "    # Forward pass\n",
    "    mask = generate_square_subsequent_mask(sequence_length).to(device)\n",
    "    output = model(input_batch, mask)\n",
    "    loss = loss_function(output.reshape(-1, vocab_size), target_batch.reshape(-1))  # Flatten the outputs and targets\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  # Print the average loss for this epoch\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(data_loader):.4f}, time: {time.time() - start_epoch:.2f} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training output\n",
    "\n",
    "```\n",
    "Epoch 1/5, Loss: 6.9044, time: 13.11 s\n",
    "Epoch 2/5, Loss: 5.1798, time: 12.51 s\n",
    "Epoch 3/5, Loss: 5.0918, time: 12.53 s\n",
    "Epoch 4/5, Loss: 5.0629, time: 12.47 s\n",
    "Epoch 5/5, Loss: 5.1361, time: 12.53 s\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_chars, temperature=1.0):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  # Convert the text into a sequence of integers using the char_to_int dictionary\n",
    "  tokens = tokenizer.tokenize(seed_text)\n",
    "  input_sequence = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  \n",
    "  # Use torch.no_grad() to prevent gradient calculations during text generation\n",
    "  with torch.no_grad():\n",
    "    # Generate 'num_chars' characters\n",
    "    for _ in range(num_chars):\n",
    "      input_tensor = torch.tensor([input_sequence[-sequence_length:]], dtype=torch.long).to(device)\n",
    "    \n",
    "      # Get the output probabilities for the next character by feeding the input tensor to the trained model\n",
    "      output = model(input_tensor)\n",
    "            \n",
    "      # Apply temperature scaling to the output logits to control the randomness of the generated text\n",
    "      output = output[:, -1, :] / temperature\n",
    "            \n",
    "      # Convert the output logits into probabilities using softmax\n",
    "      probabilities = torch.softmax(output, dim=-1)\n",
    "            \n",
    "      # Sample the index of the next character using the probabilities\n",
    "      next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "      input_sequence.append(next_token_id)\n",
    "      \n",
    "  # Generate text from the generated sequence of integers\n",
    "  generated_text = tokenizer.decode(input_sequence, clean_up_tokenization_spaces=True)\n",
    "  return generated_text\n",
    "\n",
    "seed_text = \"We are not like that.\"\n",
    "num_chars_to_generate = 300\n",
    "\n",
    "generated_text = generate_text(model, seed_text, num_chars_to_generate, temperature=0.8)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample output\n",
    "\n",
    "```\n",
    "We are not like that. began shall duty I, I think I this who I But, When,,,. pu she in I. months. mare I without I C back I heard think willme I I, I think stirring a and time of dream now content I strange you Major I I r I else, well speak be down She Box. time you. began communicate. pass. the I time back was.ten shall. I. about may comfortable at the you and, you I with I back was wisdomm content I feel many the, I I dancing, All I,llie life pu I I shallrred,, to except ‘ you,,., I came the torred Major now I I was feel I he living the Co have the are have this the had,, as many pretty I the I the I may at and she there light I ’ die ’ well this present themselves toer made,ted and I of the under I., and time about. ‘ the, well on back When and that understand about white,,, about my and acquired stall with began I I, present I, about I as dream nightten as I will much I dream themselves dream Major I duty understand what you glass had except Major of dream,, you. about I barn ‘ I something I think months was ‘ die, some something had I pass it much that first it was saying Major the the I himself duty before wish I. a and I alone such I the as well slept name duty\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
