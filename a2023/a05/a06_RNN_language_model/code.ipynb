{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It's nice to see that you already have some experience with neural networks and PyTorch. For today's lesson, we'll work on building a simple language model using an RNN in PyTorch.\n",
    "\n",
    "Here's the plan for our 60-minute session:\n",
    "\n",
    "* Briefly discuss RNNs (5 min)\n",
    "* Prepare the dataset (10 min)\n",
    "* Implement a simple RNN model in PyTorch (15 min)\n",
    "* Train the model (10 min)\n",
    "* Test the model (10 min)\n",
    "* Wrap up and discuss improvements (10 min)\n",
    "\n",
    "Let's get started with a brief discussion of RNNs.\n",
    "\n",
    "## 1. Briefly discuss RNNs (5 min)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data. They can maintain a hidden state that gets updated at each step in the sequence, making them well-suited for tasks like language modeling.\n",
    "\n",
    "For our simple language model, we'll use a character-level RNN. It will predict the next character in a sequence given the previous characters. This is a simpler task compared to predicting the next word, as GPT models do.\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "Please let me know if you are familiar with RNNs and if you have any questions about them. If you're ready to move on, I'll guide you through preparing the dataset for our language model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's begin with preparing the dataset.\n",
    "\n",
    "## 2. Prepare the dataset (10 min)\n",
    "\n",
    "For our simple language model, we'll use a text file as our dataset. You can choose any text file you like, such as a book from Project Gutenberg or a simple text file with some sample sentences.\n",
    "\n",
    "Follow these steps to prepare the dataset:\n",
    "\n",
    "* Load the text file and preprocess it (lowercase, remove special characters, etc.)\n",
    "* Create a dictionary that maps characters to integers and another dictionary that maps integers to characters. We'll use these dictionaries to convert the text to numbers and vice versa.\n",
    "* Convert the text to a sequence of integers using the character-to-integer dictionary.\n",
    "* Create input and target sequences. For each sequence of length n in the input, the corresponding target sequence should have the same length and contain the next character in the text for each character in the input sequence.\n",
    "\n",
    "Here's some code to help you get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the text\n",
    "with open(\"animal_farm.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[0:5000] # Make text shorter for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to integers\n",
    "int_text = [char_to_int[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and target sequences\n",
    "sequence_length = 50\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(int_text) - sequence_length):\n",
    "  X.append(int_text[i:i + sequence_length])\n",
    "  y.append(int_text[i + 1:i + sequence_length + 1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's move on to implementing the RNN model.\n",
    "\n",
    "### 3. Implement a simple RNN model in PyTorch (15 min)\n",
    "\n",
    "We'll create a simple RNN model using PyTorch's nn.RNN module along with a fully connected layer for the final output. Our model will have the following layers:\n",
    "\n",
    "* An embedding layer (nn.Embedding) to convert the input character integers to embeddings.\n",
    "* An RNN layer (nn.RNN) that will maintain hidden states and learn the sequence patterns.\n",
    "* A fully connected output layer (nn.Linear) that will produce the probabilities for the next character.\n",
    "\n",
    "Here's a simple implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "  def __init__(self, input_size, embed_size, hidden_size, output_size):\n",
    "    super(SimpleRNN, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, embed_size)\n",
    "    self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    x, hidden = self.rnn(x, hidden)\n",
    "    x = self.fc(x)\n",
    "    return x, hidden\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "input_size = len(chars)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "\n",
    "model = SimpleRNN(input_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our RNN model defined, we need to set up the training process. We'll need a loss function, an optimizer, and a training loop that feeds the input sequences and target sequences to the model, computes the loss, and updates the model's parameters.\n",
    "\n",
    "First, let's define the loss function and the optimizer. We'll use the Cross Entropy Loss, which is suitable for classification tasks, and the Adam optimizer for updating the model's parameters.\n",
    "\n",
    "Add the following code to your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the training loop. We'll need to perform the following steps for each epoch:\n",
    "\n",
    "* Reset the hidden state using model.init_hidden().\n",
    "* Pass the input sequence and the initial hidden state to the model.\n",
    "* Compute the loss between the model's output and the target sequence.\n",
    "* Backpropagate the loss and update the model's parameters using the optimizer.\n",
    "\n",
    "Here's the code for the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "  start_epoch = time.time()\n",
    "  \n",
    "  # Loop over the input-target pairs in the dataset\n",
    "  for i in range(0, len(X), batch_size):\n",
    "    # Get the actual batch size for the current iteration\n",
    "    actual_batch_size = min(batch_size, len(X) - i)\n",
    "    \n",
    "    # Reset the hidden state\n",
    "    hidden = model.init_hidden(actual_batch_size)\n",
    "    \n",
    "    # Detach the hidden state from its history\n",
    "    hidden.detach_()\n",
    "    \n",
    "    # Get a batch of input and target sequences\n",
    "    input_batch = torch.tensor(X[i:i+actual_batch_size], dtype=torch.long)\n",
    "    target_batch = torch.tensor(y[i:i+actual_batch_size], dtype=torch.long)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: pass the input and hidden state to the model\n",
    "    output, hidden = model(input_batch, hidden)\n",
    "    \n",
    "    # Reshape the output and target_batch\n",
    "    output = output.view(-1, output.shape[2])\n",
    "    target_batch = target_batch.view(-1)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    # Backward pass: compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "  # Print the loss for this epoch\n",
    "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, time: {time.time() - start_epoch:.2f} s')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It worked, the loss decreased from 2.4 to 0.39. It took 2 min 40 sec to run. I had to reduce the size of the input.txt file from 1 MB to 10 KB to make it run faster though.\n",
    "\n",
    "----\n",
    "I'm glad to hear that the training worked and the loss decreased! Reducing the size of the input file was a good idea to speed up the training process for this demonstration. Keep in mind that using a smaller dataset might result in a less diverse and less accurate language model. However, for the purpose of learning and understanding the RNN architecture, this should be sufficient.\n",
    "\n",
    "As you gain more experience and work with larger datasets, you can consider using more powerful hardware, like GPUs, and optimize the code further to improve training time.\n",
    "\n",
    "Now that you've trained the model, you can use it to generate new text by providing it with an initial sequence and sampling the output probabilities. This will allow you to see how well the model has learned the structure and style of the input text.\n",
    "\n",
    "You can use the following function to generate new text using your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, initial_sequence, n_chars, temperature=1.0):\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  # initial_sequence = initial_sequence.lower()  # Convert the initial sequence to lowercase\n",
    "  \n",
    "  # Only keep characters that are present in char_to_int\n",
    "  initial_sequence = ''.join(c for c in initial_sequence if c in char_to_int)\n",
    "  generated_sequence = initial_sequence\n",
    "\n",
    "  # Convert the initial sequence to a tensor\n",
    "  input_sequence = torch.tensor([char_to_int[c] for c in initial_sequence], dtype=torch.long).unsqueeze(1)\n",
    "  \n",
    "  # Initialize the hidden state\n",
    "  hidden = model.init_hidden(1)\n",
    "  \n",
    "  # Generate new characters\n",
    "  for _ in range(n_chars):\n",
    "    # Reshape input_sequence to have a sequence length of 1\n",
    "    input_sequence_reshaped = input_sequence.view(1, -1)\n",
    "    output, hidden = model(input_sequence_reshaped, hidden)\n",
    "    hidden = hidden.detach()  # Detach the hidden state\n",
    "\n",
    "    # Apply the temperature and sample the output probabilities\n",
    "    # output_dist = output.data.view(-1).div(temperature).exp()\n",
    "    output_dist = output.data[0, -1, :].div(temperature).exp()\n",
    "    \n",
    "    top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "    # Add the generated character to the sequence\n",
    "    generated_char = int_to_char[top_i.item()]\n",
    "    generated_sequence += generated_char\n",
    "\n",
    "    # Append the number for the next predicted character to the input sequence\n",
    "    input_sequence = torch.cat((input_sequence, torch.tensor([[char_to_int[generated_char]]])))\n",
    "\n",
    "  return generated_sequence\n",
    "  \n",
    "initial_sequence = \"We are not like that.\"\n",
    "n_chars = 100\n",
    "temperature = 1.0\n",
    "\n",
    "generated_text = generate_text(model, initial_sequence, n_chars, temperature)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
