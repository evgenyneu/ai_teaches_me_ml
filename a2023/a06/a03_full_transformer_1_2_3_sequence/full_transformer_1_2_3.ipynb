{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a sequence of numbres: 1, 2, 3, 4. Using the full nn.Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "      \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.tgt_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, nlayers, nlayers, d_hid, dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, use_mask: bool = True):\n",
    "        if self.tgt_mask is None or self.tgt_mask.size(0) != len(tgt):\n",
    "            device = tgt.device\n",
    "            mask = self._generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "            self.tgt_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        tgt = self.encoder(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        if (use_mask):\n",
    "          mask = self.tgt_mask\n",
    "        else:\n",
    "          mask = None\n",
    "    \n",
    "        output = self.transformer(src, tgt, tgt_mask=mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "  \n",
    "  \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer and loss function\n",
    "model = TransformerModel(ntoken=101, d_model=512, nhead=8, d_hid=2048, nlayers=6)\n",
    "lr = 0.0002  # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the number sequence and prepare input and target tensors\n",
    "num_seq = torch.arange(1, 101).unsqueeze(1)  # Shape: [100, 1]\n",
    "input_seq = torch.cat([torch.zeros(1, 1).long(), num_seq[:-1]])  # Add <start> token and remove last number\n",
    "target_seq = num_seq  # Predict the next number in the sequence\n",
    "\n",
    "# Model training\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_seq, input_seq)\n",
    "print(output.shape)\n",
    "output = output.view(-1, 101)  # Reshape for loss function\n",
    "print(output.shape)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(output, target_seq.view(-1))\n",
    "\n",
    "# Backward pass and optimization\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('Training loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
